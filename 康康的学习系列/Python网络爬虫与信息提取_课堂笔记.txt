                   《Python网络爬虫与信息提取 课堂笔记》

================================================================================
================================================================================
                        【第〇周】网络爬虫之前奏
================================================================================
                      单元0：Python语言开发工具选择
================================================================================

1.文本工具类

IDLE
Notepad++
Sublime Text 非注册免费
Vim & Emacs
Atom
Komodo Edit

2. 集成工具类

PyCharm 社区版免费 专业版收费
Wing 收费
PyDev & Eclipse
Visual Studio & PTVS
Anaconda & Spyder 收费 [科学计算和数据分析领域专用]
Canopy 免费 [科学计算和数据分析领域专用]

[注] 课程中使用IDLE、Sublime Text、PyCharm、Anaconda & Spyder作为开发环境。

================================================================================
================================================================================
                        【第一周】网络爬虫之规则
================================================================================
                          单元1：Requests库入门
================================================================================

【0. Requests库安装】
[Requests库的安装]
--------------------------------------------------------------------------------
Microsoft Windows [版本 6.1.7601]
版权所有 (c) 2009 Microsoft Corporation。保留所有权利。

C:\windows\system32>pip install requests
--------------------------------------------------------------------------------
[测试是否安装成功]
----------------------------------------------------------------------------------------------
Python 2.7.15 (v2.7.15:ca079a3ea3, Apr 30 2018, 16:30:26) [MSC v.1500 64 bit (AMD64)] on win32
Type "copyright", "credits" or "license()" for more information.
>>> import requests
>>> r = requests.get("http://www.baidu.com")
>>> r.status_code
200
>>> r.encoding='utf-8'
>>> r.text
u'<!DOCTYPE html>\r\n<!--STATUS OK--><html> <head><meta
http-equiv=content-type content=text/html;charset=utf-8><meta http-equiv=X-UA-Compatible content=IE=Edge><meta content=always name=referrer><link rel=stylesheet type=text/css href=http://s1.bdstatic.com/r/www/cache/bdorz/baidu.min.css><title>\u767e\u5ea6\u4e00\u4e0b\uff0c\u4f60\u5c31\u77e5\u9053</title></head> <body link=#0000cc> <div id=wrapper> <div id=head> <div class=head_wrapper> <div class=s_form> <div class=s_form_wrapper> <div id=lg> <img hidefocus=true src=//www.baidu.com/img/bd_logo1.png width=270 height=129> </div> <form id=form name=f action=//www.baidu.com/s class=fm> <input type=hidden name=bdorz_come value=1> <input type=hidden name=ie value=utf-8> <input type=hidden name=f value=8> <input type=hidden name=rsv_bp value=1> <input type=hidden name=rsv_idx value=1> <input type=hidden name=tn value=baidu><span class="bg s_ipt_wr"><input id=kw name=wd class=s_ipt value maxlength=255 autocomplete=off autofocus></span><span class="bg s_btn_wr"><input type=submit id=su value=\u767e\u5ea6\u4e00\u4e0b class="bg s_btn"></span> </form> </div> </div> <div id=u1> <a href=http://news.baidu.com name=tj_trnews class=mnav>\u65b0\u95fb</a> <a href=http://www.hao123.com name=tj_trhao123 class=mnav>hao123</a> <a href=http://map.baidu.com name=tj_trmap class=mnav>\u5730\u56fe</a> <a href=http://v.baidu.com name=tj_trvideo class=mnav>\u89c6\u9891</a> <a href=http://tieba.baidu.com name=tj_trtieba class=mnav>\u8d34\u5427</a> <noscript> <a href=http://www.baidu.com/bdorz/login.gif?login&amp;tpl=mn&amp;u=http%3A%2F%2Fwww.baidu.com%2f%3fbdorz_come%3d1 name=tj_login class=lb>\u767b\u5f55</a> </noscript> <script>document.write(\'<a href="http://www.baidu.com/bdorz/login.gif?login&tpl=mn&u=\'+ encodeURIComponent(window.location.href+ (window.location.search === "" ? "?" : "&")+ "bdorz_come=1")+ \'" name="tj_login" class="lb">\u767b\u5f55</a>\');</script> <a href=//www.baidu.com/more/ name=tj_briicon class=bri style="display: block;">\u66f4\u591a\u4ea7\u54c1</a> </div> </div> </div> <div id=ftCon> <div id=ftConw> <p id=lh> <a href=http://home.baidu.com>\u5173\u4e8e\u767e\u5ea6</a> <a href=http://ir.baidu.com>About Baidu</a> </p> <p id=cp>&copy;2017&nbsp;Baidu&nbsp;<a href=http://www.baidu.com/duty/>\u4f7f\u7528\u767e\u5ea6\u524d\u5fc5\u8bfb</a>&nbsp; <a href=http://jianyi.baidu.com/ class=cp-feedback>\u610f\u89c1\u53cd\u9988</a>&nbsp;\u4eacICP\u8bc1030173\u53f7&nbsp; <img src=//www.baidu.com/img/gs.gif> </p> </div> </div> </div> </body> </html>\r\n'
>>> 
----------------------------------------------------------------------------------------------
[Requests库的7个主要方法]
requests.request()
  构造一个请求，支撑以下各方法的基础方法
requests.get()
  获取HTML网页的主要方法，对应于HTTP的GET
requests.head()
  获取HTML网页头信息的方法，对应于HTTP的HEAD
requests.post()
  向HTML网页的POST方法，对应于HTTP的POST
requests.put()
  向HTML网页的PUT方法，对应于HTTP的PUT
requests.patch()
  向HTML网页的提交局部修改请求，对应于HTTP的PATCH
requests.delete()
  向HTML网页的提交删除请求，对应于HTTP的DELETE

【1. Requests库的get()方法】
r = requests.get(url, params=None, **kwargs)
url: 拟获取页面的url链接
params: url中的额外参数，字典或字节流格式，可选
**kwargs: 12个控制访问的参数

[response的5个属性]
r.status_code HTTP请求的返回状态码，200表示成功，404表示失败
r.encoding 从HTTP head中猜测的响应内容编码方式
r.apparent_encoding 从内容中分析出的响应内容编码方式（备选编码方式）
r.text HTTP响应内容的字符串形式，即url对应的页面内容
r.content HTTP响应内容的二进制形式
[理解Response的编码]
r.encoding : 如果header中不存在charset，则认为编码为ISO-8859-1
r.apparent_encoding: 根据网页内容分析出的编码方式

【2. 爬取网页的通用代码框架】
[理解Requests库的异常]
requests.ConnectionError
网络连接错误异常，如DNS查询失败、拒绝连接等
requests.HTTPError
HTTP错误异常
requests.URLRequired
URL缺失异常
requests.TooManyRedirects
超过最大重定向次数，产生重定向异常
requests.ConnectTimeout
连接远程服务器超时异常
requests.Timeout
请求URL超时，产生超时异常


r.raise_for_status()
如果不是200，产生异常requests.HTTPError

r.raise_for_status()在方法内部判断r.status_code是否等于200，不需要
增加额外的if语句，该语句便于利用try-except进行异常处理

[代码]
#!/usr/bin/env python
# -*- coding:utf-8 -*-
import requests


def get_html_text(url):
    try:
        r = requests.get(url, timeout=30)
        r.raise_for_status()
        r.encoding = r.apparent_encoding
        return r.text
    except UnicodeDecodeError:
        return "产生异常"


if __name__ == "__main__":
    http_url = "http://www.baidu.com"
    print(get_html_text(http_url))

【3. HTTP协议及Requests库方法】
[HTTP协议]
HTTP，Hypertext Transfer Protocol，超文本传输协议
  HTTP是一个基于“请求与响应”模式的、无状态的应用层协议
  HTTP协议采用URL作为定位网络资源的标识，URL格式如下：
          http://host[:port][path]

host: 合法的Internet主机域名或IP地址
port: 端口号，缺省端口为80
path: 请求资源的路径
HTTP，Hypertext Transfer Protocol，超文本传输协议
HTTP URL实例：
http://www.bit.edu.cn
http://220.181.111.188/duty
HTTP URL的理解：
URL是通过HTTP协议存取资源的Internet路径，一个URL对应一个数据资源

[HTTP协议对资源的操作]
方法 说明
GET	请求获取URL位置的资源
HEAD 请求获取URL位置资源的响应消息报告，即获得该资源的头部信息
POST 请求向URL位置的资源后附加新的数据
PUT 请求向URL位置存储一个资源，覆盖原URL位置的资源
PATCH 请求局部更新URL位置的资源，即改变该处资源的部分内容
DELETE 请求删除URL位置存储的资源

获取：GET HEAD
提交：PUT POST PATCH DELETE
通过URL和命令管理资源，操作独立无状态，网络通道及服务器成为了黑盒子

[理解PATCH与PUT的区别]
假设URL位置有一组数据UserInfo，包括UserID、UserName等20个字段
需求：用户修改了UserName，其他不变
> 采用PATCH，仅向URL提交UserName的局部更新请求
> 采用PUT，必须将所有20个字段一并提交到URL，未提交字段被删除
PATCH的最主要好处：节省网络带宽
[HTTP协议与Requests库]
HTTP协议方法 Requests库方法 功能一致性
GET requests.get() 一致
HEAD requests.head() 一致
POST requests.post() 一致
PUT requests.put() 一致
PATCH requests.patch() 一致
DELETE requests.delete() 一致

[Requests库的head()方法]
>>> r = requests.head('http://httpbin.org/get')
>>> r.headers
{'Content‐Length': '238', 'Access‐Control‐Allow‐Origin': '*', 'Access‐
Control‐Allow‐Credentials': 'true', 'Content‐Type':
'application/json', 'Server': 'nginx', 'Connection': 'keep‐alive',
'Date': 'Sat, 18 Feb 2017 12:07:44 GMT'}
>>> r.text
''
[Requests库的post()方法]
>>> payload = {'key1': 'value1', 'key2': 'value2'}
>>> r = requests.post('http://httpbin.org/post', data = payload)
>>> print(r.text)
{ ...
"form": {
"key2": "value2",
"key1": "value1"
},
}
向URL POST一个字典自动编码为form（表单）

>>> r = requests.post('http://httpbin.org/post', data = 'ABC')
>>> print(r.text)
{ ...
"data": "ABC"
"form": {},
}
向URL POST一个字符串自动编码为data

[Requests库的put()方法]
>>> payload = {'key1': 'value1', 'key2': 'value2'}
>>> r = requests.put('http://httpbin.org/put', data = payload)
>>> print(r.text)
{ ...
"form": {
"key2": "value2",
"key1": "value1"
},
}

【4. Requests库主要方法解析】
[Requests库的7个主要方法]
方法 说明
requests.request() 构造一个请求，支撑以下各方法的基础方法
requests.get() 获取HTML网页的主要方法，对应于HTTP的GET
requests.head() 获取HTML网页头信息的方法，对应于HTTP的HEAD
requests.post() 向HTML网页提交POST请求的方法，对应于HTTP的POST
requests.put() 向HTML网页提交PUT请求的方法，对应于HTTP的PUT
requests.patch() 向HTML网页提交局部修改请求，对应于HTTP的PATCH
requests.delete() 向HTML页面提交删除请求，对应于HTTP的DELETE

[1. requests.request() 方法]

requests.request(method, url, **kwargs)
> method : 请求方式，对应get/put/post等7种
> url : 拟获取页面的url链接
> **kwargs: 控制访问的参数，共13个

requests.request(method, url, **kwargs)
> method : 请求方式
r = requests.request('GET', url, **kwargs)
r = requests.request('HEAD', url, **kwargs)
r = requests.request('POST', url, **kwargs)
r = requests.request('PUT', url, **kwargs)
r = requests.request('PATCH', url, **kwargs)
r = requests.request('delete', url, **kwargs)
r = requests.request('OPTIONS', url, **kwargs)

requests.request(method, url, **kwargs)
> **kwargs: 控制访问的参数，均为可选项

1. params 参数
>> params : 字典或字节序列，作为参数增加到url中

例：
>>> kv = {'key1': 'value1', 'key2': 'value2'}
>>> r = requests.request('GET', 'http://python123.io/ws', params=kv)
>>> print(r.url)
http://python123.io/ws?key1=value1&key2=value2

2. data 参数
>> data : 字典、字节序列或文件对象，作为Request的内容

例：
>>> kv = {'key1': 'value1', 'key2': 'value2'}
>>> r = requests.request('POST', 'http://python123.io/ws', data=kv)
>>> body = '主体内容'
>>> r = requests.request('POST', 'http://python123.io/ws', data=body)

3. json 参数
>> json : JSON格式的数据，作为Request的内容
例：
>>> kv = {'key1': 'value1'}
>>> r = requests.request('POST', 'http://python123.io/ws', json=kv)

4. headers 参数
>> headers : 字典，HTTP定制头
例：
>>> hd = {'user‐agent': 'Chrome/10'}
>>> r = requests.request('POST', 'http://python123.io/ws', headers=hd)

5. cookies 参数
>> cookies : 字典或CookieJar，Request中的cookie

6. auth 参数
>> auth : 元组，支持HTTP认证功能

7. files 参数
>> files : 字典类型，传输文件
例：
>>> fs = {'file': open('data.xls', 'rb')}
>>> r = requests.request('POST', 'http://python123.io/ws', files=fs)

8. timeout 参数
>> timeout : 设定超时时间，秒为单位
例：
>>> r = requests.request('GET', 'http://www.baidu.com', timeout=10)

9. proxies 参数
>>proxies : 字典类型，设定访问代理服务器，可以增加登录认证
例：
>>> pxs = { 'http': 'http://user:pass@10.10.10.1:1234'
'https': 'https://10.10.10.1:4321' }
>>> r = requests.request('GET', 'http://www.baidu.com', proxies=pxs)

10. allow_redirects 参数
>> allow_redirects : True/False，默认为True，重定向开关

11. stream 参数
>> stream : True/False，默认为True，获取内容立即下载开关

12: verify 参数
>> verify : True/False，默认为True，认证SSL证书开关

13. cert 参数
>> cert : 本地SSL证书路径

[2. requests.get() 方法]

requests.get(url, params=None, **kwargs)
> url : 拟获取页面的url链接
> params : url中的额外参数，字典或字节流格式，可选
> **kwargs: 12个控制访问的参数

[3. requests.head() 方法]
requests.head(url, **kwargs)
> url : 拟获取页面的url链接
> **kwargs: 12个控制访问的参数

[4. requests.post() 方法]

requests.post(url, data=None, json=None, **kwargs)
> url : 拟更新页面的url链接
> data : 字典、字节序列或文件，Request的内容
> json : JSON格式的数据，Request的内容
> **kwargs: 12个控制访问的参数

[5. requests.put() 方法]

requests.put(url, data=None, **kwargs)
> url : 拟更新页面的url链接
> data : 字典、字节序列或文件，Request的内容
> **kwargs: 12个控制访问的参数

[6. requests.patch() 方法]
requests.patch(url, data=None, **kwargs)
> url : 拟更新页面的url链接
> data : 字典、字节序列或文件，Request的内容
> **kwargs: 12个控制访问的参数

[7. requests.delete() 方法]
requests.delete(url, **kwargs)
> url : 拟删除页面的url链接
> **kwargs: 12个控制访问的参数

[8. requests.get() 方法]
requests.get(url, params=None, **kwargs)
> url : 拟获取页面的url链接
> params : url中的额外参数，字典或字节流格式，可选
> **kwargs: 12个控制访问的参数

【5. 单元小结】

[requests库重要方法]
requests.request()
requests.get()
requests.head()
requests.post()
requests.put()
requests.patch()
requests.delete()

[爬取网页的通用代码框架]
def get_html_text(url):
    try:
        r = requests.get(url, timeout=30)
        r.raise_for_status()
        r.encoding = r.apparent_encoding
        return r.text
    except UnicodeDecodeError:
        return "产生异常"

================================================================================
                      单元2：网络爬虫的"盗亦有道"
================================================================================

【0. 网络爬虫引发的问题】
[网络爬虫的尺寸]

|--------------------------------------
| Requests库 
| 小规模，数据量小，爬取速度不明显
| > 90%
| 爬取网页 玩转网页
|--------------------------------------
|Scrapy库
|中规模，数据规模较大，爬取速度敏感
|
|爬取网站 爬取系列网站
|--------------------------------------
|定制开发
|大规模，搜索引擎，爬取速度关键
|
|爬取全网
|--------------------------------------
V

[网络爬虫的"性能骚扰"]
Web服务器默认接收人类访问
受限于编写水平和目的，网络爬虫将会为Web服务器带来巨大的资源开销

[网络爬虫的法律风险]
服务器上的数据有产权归属
网络爬虫获取数据后牟利将带来法律风险

[网络爬虫的隐私泄露]
网络爬虫可能具备突破简单访问控制的能力，获得被保护数据
从而泄露个人隐私

[网络爬虫的限制]
来源审查：判断User-Agent进行限制
检查来访HTTP协议头的User-Agent域，只响应浏览器或友好爬虫的访问
发布公告：Robots协议
告知所有爬虫网站的爬取策略，要求爬虫遵守

【1. Robots协议】
Robots Exclusion Standard,网络爬虫排除标准
作用:
网站告知网络爬虫哪些页面可以抓取,哪些不行
形式:
在网站根目录下的robots.txt文件

[案例：京东的Robots协议]
https://www.jd.com/robots.txt
User‐agent: *
Disallow: /?*
Disallow: /pop/*.html
Disallow: /pinpai/*.html?*
User‐agent: EtaoSpider
Disallow: /
User‐agent: HuihuiSpider
Disallow: /
User‐agent: GwdangSpider
Disallow: /
User‐agent: WochachaSpider
Disallow: /

# 注释,*代表所有,/代表根目录
User‐agent: *
Disallow: /
Robots协议基本语法

[案例：真实的Robots协议]
http://www.baidu.com/robots.txt
http://news.sina.com.cn/robots.txt
http://www.qq.com/robots.txt
http://news.qq.com/robots.txt
http://www.moe.edu.cn/robots.txt (无robots协议)

【2. Robots协议的遵守方式】

实际操作中,该如何遵守Robots协议?

[Robots协议的使用]
网络爬虫:
自动或人工识别robots.txt,再进行内容爬取
约束性:
Robots协议是建议但非约束性,网络爬虫可以不遵守,但存在法律风险

[对Robots协议的理解]

|--------------------------------------
| 访问量很小：可以遵守 
| 访问量较大：建议遵守 
|
| 爬取网页 玩转网页
|--------------------------------------
|非商业且偶尔：建议遵守
|商业利益：必须遵守
|
|爬取网站 爬取系列网站
|--------------------------------------
|必须遵守
|
|爬取全网
|--------------------------------------
V
原则:类人行为可不参考Robots协议


【3. 单元小结】
# 注释,*代表所有,/代表根目录
User‐agent: *
Disallow: /
Robots协议基本语法

Robots协议的使用原则


